[Data]
bert_model = 'bert-base-multilingual-cased'

[Network]
n_embed = 0
n_char_embed = 50
n_feat_embed = 0
n_bert_layers = 0
n_attentions = True
attention_layer = 6
bert_fine_tune = False
use_hidden_states = True
mix_dropout = .1
embed_dropout = .33
n_lstm_hidden = 400
n_lstm_layers = 2
lstm_dropout = .33
token_dropout = 0.0
n_mlp_arc = 500
n_mlp_rel = 100
mlp_dropout = .33

[Optimizer]
optimizer = 'adam'
lr = 2e-3
mu = .9
nu = .99
epsilon = 1e-12
clip = 5.0
decay = .75
decay_steps = 5000
accumulation_steps = 1
evaluate_in_training = True
warmup_steps_ratio = .1

[Run]
batch_size = 5000
epochs = 1000
patience = 20
min_freq = 2
fix_len = 20
